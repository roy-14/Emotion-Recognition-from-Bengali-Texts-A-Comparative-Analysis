# -*- coding: utf-8 -*-
"""Excel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10EnLpBWInHhbgo8JM3jKyoYGkdw54XyM
"""

from google.colab import drive
drive.mount('/content/drive')



""" **Git Clonning**"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/ashwoolford/bnltk.git

# %cd bnltk

import bnltk

"""**Libraries**"""

!pip install nltk
!pip install bnlp_toolkit
!pip install spacy
!pip install langid
!pip install tensorflow
!pip install scikit-learn
!pip install gensim
!pip install fasttext
!pip install torch
!pip install keras
!pip install tensorflow scikit-learn
!pip install keras scikit-learn
!pip install pandas openpyxl
!pip install matplotlib seaborn

"""**Load Dataset**"""

import pandas as pd

path ="/content/drive/MyDrive/ED/Datasets/dataset emotion.xlsx"
df = pd.read_excel(path)

df.head()

"""**Dataset Pre-process**

*Remove Punctuation*
"""

import pandas as pd
import re

# Load the dataset
df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/dataset emotion.xlsx')

df.head()

def cleaning_data(row):
    text = re.sub('[^\u0980-\u09FF]',' ',str(row)) #removing unnecessary punctuation
    return text

# Apply the function to the Text column
df['Text'] = df['Text'].apply(cleaning_data)

df.head()





# Define a list of Bengali stopwords
bengali_stopwords = ["না", "করে", "এই", "আমার", "আর", "আমি","থেকে","তার","ও","জন্য"]



"""*Tokenization*"""

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize

# Download the necessary NLTK models (if needed)
nltk.download('punkt')

# Load the dataset
# df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/remove_punct_dataset.xlsx')

# Define a function to tokenize text
def tokenize_text(text):
    return word_tokenize(text)

# Apply the function to the Text column
df['Tokenized_Text'] = df['Text'].apply(tokenize_text)

# Save the tokenized DataFrame to a new Excel file
df.to_excel('/content/drive/MyDrive/ED/Datasets/tokenized_dataset_emotion.xlsx', index=False)

# Display some of the tokenized data
print(df.head())

import pandas as pd
from collections import Counter

# Load dataset
df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/tokenized_dataset_emotion.xlsx')

# Process text: combining, lowering case, splitting into words
text = ' '.join(df['Text'].dropna().astype(str)).lower()
words = text.split()

# Count and display frequencies
word_counts = Counter(words)
print(word_counts.most_common(10))  # Display top 10 words

"""*Stop Words Removal*"""

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

# Define Bengali stopwords
bengali_stopwords = ["না", "করে", "এই", "আমার", "আর", "আমি","থেকে","তার","ও","জন্য"]

def tokenize_and_remove_stopwords(text):
    tokens = word_tokenize(text)
    return [token for token in tokens if token not in bengali_stopwords]


# Apply tokenization and stopword removal
df['Tokenized_Text'] = df['Text'].apply(tokenize_and_remove_stopwords)

# Save the updated DataFrame to an Excel file
output_file_path = '/content/drive/MyDrive/ED/Datasets/tokenized_dataset_emotion.xlsx'
df.to_excel(output_file_path, index=False)  # Set index=False to not save row indices

df.head()



"""*Split Dataset*"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Load your dataset
df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/tokenized_dataset_emotion.xlsx')

# Split into train and remaining datasets with 70% data in train and 30% for remaining
train_df, remaining_df = train_test_split(df, test_size=0.3, random_state=42)

# Split the remaining data into validation and test sets each containing 50% of the remaining data
val_df, test_df = train_test_split(remaining_df, test_size=0.5, random_state=42)

# Define file paths
train_file_path = '/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx'
val_file_path = '/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx'
test_file_path = '/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx'

# Save to Excel files
train_df.to_excel(train_file_path, index=False)
val_df.to_excel(val_file_path, index=False)
test_df.to_excel(test_file_path, index=False)

# Print the sizes of each dataset
print("Training Set Size: ", len(train_df))
print("Validation Set Size: ", len(val_df))
print("Testing Set Size: ", len(test_df))

"""**Feature Extraction for ML models**

*Term Frequency-Inverse Document Frequency (TF-IDF)*
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Function to read datasets
def load_data(filepath):
    return pd.read_excel(filepath)

# Load datasets
train_df = load_data('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = load_data('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = load_data('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Combine and then split datasets to ensure uniform vocabulary
combined_df = pd.concat([train_df, test_df, val_df])
train_texts, other_texts, train_labels, other_labels = train_test_split(
    combined_df['Tokenized_Text'], combined_df['Label'], test_size=0.3, random_state=42)

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limit number of features to 1000, adjust as needed

# Fit and transform the training data
tfidf_train = tfidf_vectorizer.fit_transform(train_texts)

# Only transform the test and validation data
tfidf_test = tfidf_vectorizer.transform(test_df['Tokenized_Text'])
tfidf_val = tfidf_vectorizer.transform(val_df['Tokenized_Text'])

# The datasets are now ready to be used with ML models
# tfidf_train, tfidf_test, and tfidf_val are the datasets to use
# For demonstration, here's how to view the feature names and the shape of the transformed data
print("Feature names:", tfidf_vectorizer.get_feature_names_out())
print("Shape of Train TF-IDF Matrix:", tfidf_train.shape)
print("Shape of Test TF-IDF Matrix:", tfidf_test.shape)
print("Shape of Validation TF-IDF Matrix:", tfidf_val.shape)

"""*Bag of Words (BoW)*"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

# Function to load data from Excel file
def load_data(filepath):
    return pd.read_excel(filepath, usecols=["Tokenized_Text", "Label"])

# Load datasets
train_df = load_data('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = load_data('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = load_data('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Initialize the CountVectorizer (BoW model)
vectorizer = CountVectorizer()

# Fit the vectorizer on the training data
vectorizer.fit(train_df['Tokenized_Text'])

# Transform the datasets
X_train = vectorizer.transform(train_df['Tokenized_Text'])
X_test = vectorizer.transform(test_df['Tokenized_Text'])
X_val = vectorizer.transform(val_df['Tokenized_Text'])

# Get the labels
y_train = train_df['Label']
y_test = test_df['Label']
y_val = val_df['Label']

# Example: Print the shape of the training data and feature names
print("Shape of training data:", X_train.shape)
print("Feature names:", vectorizer.get_feature_names_out())

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

# Load the dataset
df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/tokenized_dataset_emotion.xlsx')

# Make sure text is in the correct format
df['Tokenized_Text'] = df['Tokenized_Text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)

# Print some of the text to verify its format and content
print(df['Tokenized_Text'].head(20))

# Initialize a Count Vectorizer with basic settings
count_vectorizer = CountVectorizer()

# Try to fit and transform the text data to create BoW features
try:
    bow_matrix = count_vectorizer.fit_transform(df['Tokenized_Text'])
    print("BoW matrix shape:", bow_matrix.shape)
except ValueError as e:
    print("Error encountered:", e)
    print("Inspect the output above to ensure text data is valid.")

# Optional: Convert to DataFrame for easier manipulation and viewing
if 'bow_matrix' in locals():  # Check if bow_matrix was successfully created
    bow_df = pd.DataFrame(bow_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())
    # Display some of the features
    print(bow_df.head())

"""**Machine Learning Models**

Logistic Regression

*Logistic Regression on TF-IDF*
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt  # Import matplotlib for plotting
import seaborn as sns           # Import seaborn for heatmap visualization

# Load datasets
def load_data(filepath):
    return pd.read_excel(filepath, usecols=["Tokenized_Text", "Label"])

train_df = load_data('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = load_data('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = load_data('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed

# Fit the vectorizer on the training data and transform the text
X_train = tfidf.fit_transform(train_df['Tokenized_Text'])
X_test = tfidf.transform(test_df['Tokenized_Text'])
X_val = tfidf.transform(val_df['Tokenized_Text'])

# Encoding labels
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(train_df['Label'])
y_test = label_encoder.transform(test_df['Label'])
y_val = label_encoder.transform(val_df['Label'])

# Initialize Logistic Regression model
log_reg = LogisticRegression(solver='lbfgs', max_iter=400, C=1, penalty='l2')

# Fit the model
log_reg.fit(X_train, y_train)

# Predict on validation and test sets
val_predictions = log_reg.predict(X_val)
test_predictions = log_reg.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, test_predictions)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, test_predictions))

# Calculate the confusion matrix
conf_mat = confusion_matrix(y_test, test_predictions)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()







"""*Logistic Regression on BoW*"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt  # Import for plotting
import seaborn as sns           # Import for heatmap visualization

# Load the data
def load_data(filepath):
    return pd.read_excel(filepath, usecols=["Tokenized_Text", "Label"])

train_df = load_data('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = load_data('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = load_data('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Combine train and validation for training if needed or use validation for model tuning
combined_train_df = pd.concat([train_df, val_df])

# Initialize CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the training data
X_train = vectorizer.fit_transform(combined_train_df['Tokenized_Text'])
y_train = combined_train_df['Label']

# Transform the test data
X_test = vectorizer.transform(test_df['Tokenized_Text'])
y_test = test_df['Label']

# Initialize and train the Logistic Regression model
model = LogisticRegression(solver='lbfgs', max_iter=400, penalty='l2', C=1)
model.fit(X_train, y_train)

# Predict on the test data
predicted_labels = model.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, predicted_labels)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, predicted_labels))

# Calculate the confusion matrix
conf_mat = confusion_matrix(y_test, predicted_labels)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Reds', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""SVM

*SVM on TF-IDF*
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt  # Import for plotting
import seaborn as sns           # Import for heatmap visualization

# Function to load data from an Excel file
def load_data(filepath):
    return pd.read_excel(filepath, usecols=["Tokenized_Text", "Label"])

# Load datasets
train_df = load_data('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = load_data('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = load_data('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Initialize the TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed

# Fit the vectorizer on the training data and transform the text
X_train = tfidf.fit_transform(train_df['Tokenized_Text'])
X_test = tfidf.transform(test_df['Tokenized_Text'])
X_val = tfidf.transform(val_df['Tokenized_Text'])

# Encoding labels
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(train_df['Label'])
y_test = label_encoder.transform(test_df['Label'])
y_val = label_encoder.transform(val_df['Label'])

# Initialize the Random Forest classifier
random_forest = RandomForestClassifier(criterion='gini', n_estimators=100, min_samples_split=2, random_state=0)

# Fit the model
random_forest.fit(X_train, y_train)

# Predict on the test dataset
test_predictions = random_forest.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, test_predictions)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, test_predictions))

# Calculate the confusion matrix
conf_mat = confusion_matrix(y_test, test_predictions)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Greens', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()



"""*SVM on BoW*"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt  # Import for plotting
import seaborn as sns           # Import for heatmap visualization

# Function to load data from an Excel file
def load_data(filepath, use_cols):
    return pd.read_excel(filepath, usecols=use_cols)

# Load datasets
train_df = load_data('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx', ["Tokenized_Text", "Label"])
test_df = load_data('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx', ["Tokenized_Text", "Label"])
val_df = load_data('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx', ["Tokenized_Text", "Label"])

# Combine train and validation for more data during training
combined_train_df = pd.concat([train_df, val_df])

# Initialize CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the combined training data
X_train = vectorizer.fit_transform(combined_train_df['Tokenized_Text'])
y_train = combined_train_df['Label']

# Transform the test data
X_test = vectorizer.transform(test_df['Tokenized_Text'])
y_test = test_df['Label']

# Initialize the SVM classifier
svm_classifier = SVC(kernel='rbf', gamma='scale', tol=1, random_state=0)

# Train the SVM model
svm_classifier.fit(X_train, y_train)

# Predict on the test data
predicted_labels = svm_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predicted_labels)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, predicted_labels))

# Calculate the confusion matrix
conf_mat = confusion_matrix(y_test, predicted_labels)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Reds', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()



"""Random Forest

*Random Forest on TF-IDF*
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt  # Added for plotting
import seaborn as sns           # Added for heatmap visualization

# Function to load data from an Excel file
def load_data(filepath):
    return pd.read_excel(filepath, usecols=["Tokenized_Text", "Label"])

# Load datasets
train_df = load_data('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = load_data('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = load_data('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Initialize the TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed

# Fit the vectorizer on the training data and transform the text
X_train = tfidf.fit_transform(train_df['Tokenized_Text'])
X_test = tfidf.transform(test_df['Tokenized_Text'])
X_val = tfidf.transform(val_df['Tokenized_Text'])

# Encoding labels
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(train_df['Label'])
y_test = label_encoder.transform(test_df['Label'])
y_val = label_encoder.transform(val_df['Label'])

# Initialize the Random Forest classifier
random_forest = RandomForestClassifier(criterion='gini', n_estimators=100, min_samples_split=2, random_state=0)

# Fit the model
random_forest.fit(X_train, y_train)

# Predict on the test dataset
test_predictions = random_forest.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, test_predictions)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, test_predictions))

# Calculate the confusion matrix
conf_mat = confusion_matrix(y_test, test_predictions)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""*Random Forest on BoW*"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt  # Added for plotting
import seaborn as sns           # Added for heatmap visualization

# Function to load data from an Excel file
def load_data(filepath, use_cols):
    return pd.read_excel(filepath, usecols=use_cols)

# Load datasets
train_df = load_data('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx', ["Tokenized_Text", "Label"])
test_df = load_data('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx', ["Tokenized_Text", "Label"])
val_df = load_data('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx', ["Tokenized_Text", "Label"])

# Combine train and validation for more robust training
combined_train_df = pd.concat([train_df, val_df])

# Initialize CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the combined training data
X_train = vectorizer.fit_transform(combined_train_df['Tokenized_Text'])
y_train = combined_train_df['Label']

# Transform the test data
X_test = vectorizer.transform(test_df['Tokenized_Text'])
y_test = test_df['Label']

# Initialize the Random Forest classifier
random_forest = RandomForestClassifier(criterion='gini', n_estimators=100, min_samples_split=2, random_state=0)

# Train the Random Forest model
random_forest.fit(X_train, y_train)

# Predict on the test data
predicted_labels = random_forest.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predicted_labels)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, predicted_labels))

# Calculate the confusion matrix
conf_mat = confusion_matrix(y_test, predicted_labels)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Reds', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""Multinomial Naive Bayes

*MNB on TF-IDF*
"""



import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt  # Added for plotting
import seaborn as sns           # Added for heatmap visualization

# Function to load data from an Excel file
def load_data(filepath, columns):
    return pd.read_excel(filepath, usecols=columns)

# Load datasets
train_df = load_data('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx', ["Tokenized_Text", "Label"])
test_df = load_data('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx', ["Tokenized_Text", "Label"])
val_df = load_data('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx', ["Tokenized_Text", "Label"])

# Initialize the TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed

# Fit the vectorizer on the training data and transform the text
X_train = tfidf.fit_transform(train_df['Tokenized_Text'])
X_test = tfidf.transform(test_df['Tokenized_Text'])
X_val = tfidf.transform(val_df['Tokenized_Text'])

# Encoding labels
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(train_df['Label'])
y_test = label_encoder.transform(test_df['Label'])
y_val = label_encoder.transform(val_df['Label'])

# Initialize the Multinomial Naive Bayes classifier
naive_bayes_classifier = MultinomialNB(alpha=1.0, fit_prior=True)

# Fit the model
naive_bayes_classifier.fit(X_train, y_train)

# Predict on the test dataset
test_predictions = naive_bayes_classifier.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, test_predictions)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, test_predictions))

# Calculate the confusion matrix
conf_mat = confusion_matrix(y_test, test_predictions)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Greens', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""*MNB on BoW*"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt  # Added for plotting
import seaborn as sns           # Added for heatmap visualization

# Function to load data from an Excel file
def load_data(filepath, use_cols):
    return pd.read_excel(filepath, usecols=use_cols)

# Load datasets
train_df = load_data('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx', ["Tokenized_Text", "Label"])
test_df = load_data('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx', ["Tokenized_Text", "Label"])
val_df = load_data('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx', ["Tokenized_Text", "Label"])

# Combine train and validation datasets
combined_train_df = pd.concat([train_df, val_df])

# Initialize CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the combined training data
X_train = vectorizer.fit_transform(combined_train_df['Tokenized_Text'])
y_train = combined_train_df['Label']

# Transform the test data
X_test = vectorizer.transform(test_df['Tokenized_Text'])
y_test = test_df['Label']

# Initialize Multinomial Naive Bayes classifier with parameters
nb_classifier = MultinomialNB(alpha=1.0, fit_prior=True)

# Train the Multinomial Naive Bayes model
nb_classifier.fit(X_train, y_train)

# Predict on the test data
predicted_labels = nb_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predicted_labels)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, predicted_labels))

# Calculate the confusion matrix
conf_mat = confusion_matrix(y_test, predicted_labels)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""**Feature Extraction for DL models**

*Word2Vec*
"""

import gensim
from gensim.models import Word2Vec
import pandas as pd

# Load your dataset
df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
texts = df['Tokenized_Text'].apply(lambda x: x.split())  # Ensure texts are tokenized

# Train a Word2Vec model
model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)

# Save the model for later use
model.save("/content/drive/MyDrive/ED/Datasets/word2vec_bengali.model")

# To use the model
word_vectors = model.wv

"""*FastText*"""

import gensim
from gensim.models import FastText
import pandas as pd

# Load your dataset
df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
texts = df['Tokenized_Text'].apply(lambda x: x.split())  # Ensure texts are tokenized

# Train a FastText model
fasttext_model = FastText(sentences=texts, vector_size=100, window=5, min_count=1, workers=4, sg=1)  # sg=1 for skip-gram model

# Save the model for later use
fasttext_model.save("/content/drive/MyDrive/ED/Datasets/fasttext_bengali.model")

# To use the model
word_vectors = fasttext_model.wv

"""**Models of DL**

**CNN**

*CNN on Word2Vec*
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from gensim.models import Word2Vec
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt  # Added for plotting
import seaborn as sns           # Added for heatmap visualization

# Load Word2Vec model
w2v_model = Word2Vec.load('/content/drive/MyDrive/ED/Datasets/word2vec_bengali.model')
embedding_matrix = w2v_model.wv.vectors
vocab_size, embedding_dim = embedding_matrix.shape

# Model parameters
max_length = 100  # Adjust based on your dataset
filter_size = 7
num_units = 64
batch_size = 16
learning_rate = 0.001

# Load datasets
train_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Prepare text data
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(train_df['Tokenized_Text'])
def get_sequences(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')
    return padded
train_sequences = get_sequences(train_df['Tokenized_Text'])
test_sequences = get_sequences(test_df['Tokenized_Text'])
val_sequences = get_sequences(val_df['Tokenized_Text'])

# Labels
train_labels = train_df['Label'].values
test_labels = test_df['Label'].values
val_labels = val_df['Label'].values

# Define CNN model
model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Conv1D(num_units, filter_size, activation='relu'),
    MaxPooling1D(pool_size=2),
    GlobalMaxPooling1D(),
    Dense(num_units, activation='relu'),
    Dense(len(np.unique(train_labels)), activation='softmax')
])

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
history = model.fit(train_sequences, train_labels, epochs=30, validation_data=(val_sequences, val_labels), batch_size=batch_size)

# Evaluate the model on test data
predictions = model.predict(test_sequences)
predicted_labels = np.argmax(predictions, axis=1)

# Metrics
accuracy = accuracy_score(test_labels, predicted_labels)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(test_labels, predicted_labels))

# Calculate the confusion matrix
conf_mat = confusion_matrix(test_labels, predicted_labels)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(train_labels), yticklabels=np.unique(train_labels))
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""*CNN on FastText*"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from gensim.models import FastText
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt  # Added for plotting
import seaborn as sns           # Added for heatmap visualization

# Load FastText model and its vectors
fasttext_model_path = '/content/drive/MyDrive/ED/Datasets/fasttext_bengali.model'
fasttext_model = FastText.load(fasttext_model_path)
embedding_matrix = np.load('/content/drive/MyDrive/ED/Datasets/fasttext_bengali.model.wv.vectors_ngrams.npy')
vocab_size, embedding_dim = embedding_matrix.shape

# Model parameters
max_length = 100
filter_size = 7
num_units = 64
batch_size = 16
learning_rate = 0.001

# Load datasets
train_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Prepare text data
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(train_df['Tokenized_Text'])

def get_sequences(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')
    return padded

train_sequences = get_sequences(train_df['Tokenized_Text'])
test_sequences = get_sequences(test_df['Tokenized_Text'])
val_sequences = get_sequences(val_df['Tokenized_Text'])

# Labels
train_labels = train_df['Label'].values
test_labels = test_df['Label'].values
val_labels = val_df['Label'].values

# Define CNN model
model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Conv1D(num_units, filter_size, activation='relu'),
    MaxPooling1D(pool_size=2),
    GlobalMaxPooling1D(),
    Dense(num_units, activation='relu'),
    Dense(len(np.unique(train_labels)), activation='softmax')
])

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
history = model.fit(train_sequences, train_labels, epochs=30, validation_data=(val_sequences, val_labels), batch_size=batch_size)

# Evaluate the model on test data
predictions = model.predict(test_sequences)
predicted_labels = np.argmax(predictions, axis=1)

# Metrics
accuracy = accuracy_score(test_labels, predicted_labels)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(test_labels, predicted_labels))

# Calculate the confusion matrix
conf_mat = confusion_matrix(test_labels, predicted_labels)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Reds', xticklabels=np.unique(train_labels), yticklabels=np.unique(train_labels))
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""**BiLSTM**

*BiLSTM on Word2Vec*
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from gensim.models import Word2Vec
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt  # Added for plotting
import seaborn as sns           # Added for heatmap visualization

# Load Word2Vec model
word2vec_model_path = '/content/drive/MyDrive/ED/Datasets/word2vec_bengali.model'
word2vec_model = Word2Vec.load(word2vec_model_path)
embedding_matrix = word2vec_model.wv.vectors
vocab_size, embedding_dim = embedding_matrix.shape

# Model parameters
max_length = 100
num_units = 32
batch_size = 16
learning_rate = 0.001
epochs = 30

# Load datasets
train_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Prepare text data
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(train_df['Tokenized_Text'])

def get_sequences(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')
    return padded

train_sequences = get_sequences(train_df['Tokenized_Text'])
test_sequences = get_sequences(test_df['Tokenized_Text'])
val_sequences = get_sequences(val_df['Tokenized_Text'])

# Labels
train_labels = train_df['Label'].values
test_labels = test_df['Label'].values
val_labels = val_df['Label'].values

# Define BiLSTM model
model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Bidirectional(LSTM(num_units)),
    Dense(16, activation='relu'),
    Dense(len(np.unique(train_labels)), activation='softmax')
])

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
history = model.fit(train_sequences, train_labels, epochs=epochs, validation_data=(val_sequences, val_labels), batch_size=batch_size)

# Evaluate the model on test data
predictions = model.predict(test_sequences)
predicted_labels = np.argmax(predictions, axis=1)

# Metrics
accuracy = accuracy_score(test_labels, predicted_labels)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(test_labels, predicted_labels))

# Calculate the confusion matrix
conf_mat = confusion_matrix(test_labels, predicted_labels)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Greens', xticklabels=np.unique(train_labels), yticklabels=np.unique(train_labels))
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""*BiLSTM on FastText*"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from gensim.models import FastText
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt  # Import for plotting
import seaborn as sns           # Import for heatmap visualization

# Load FastText model and its vectors
fasttext_model_path = '/content/drive/MyDrive/ED/Datasets/fasttext_bengali.model'
fasttext_model = FastText.load(fasttext_model_path)
embedding_matrix = np.load('/content/drive/MyDrive/ED/Datasets/fasttext_bengali.model.wv.vectors_ngrams.npy')
vocab_size, embedding_dim = embedding_matrix.shape

# Model parameters
max_length = 100
num_units = 32
batch_size = 16
learning_rate = 0.001
epochs = 30

# Load datasets
train_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Prepare text data
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(train_df['Tokenized_Text'])
def get_sequences(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')
    return padded
train_sequences = get_sequences(train_df['Tokenized_Text'])
test_sequences = get_sequences(test_df['Tokenized_Text'])
val_sequences = get_sequences(val_df['Tokenized_Text'])

# Labels
train_labels = train_df['Label'].values
test_labels = test_df['Label'].values
val_labels = val_df['Label'].values

# Define BiLSTM model
model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Bidirectional(LSTM(num_units)),
    Dense(16, activation='relu'),
    Dense(len(np.unique(train_labels)), activation='softmax')
])

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
history = model.fit(train_sequences, train_labels, epochs=epochs, validation_data=(val_sequences, val_labels), batch_size=batch_size)

# Evaluate the model on test data
predictions = model.predict(test_sequences)
predicted_labels = np.argmax(predictions, axis=1)

# Metrics
accuracy = accuracy_score(test_labels, predicted_labels)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(test_labels, predicted_labels))

# Calculate the confusion matrix
conf_mat = confusion_matrix(test_labels, predicted_labels)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(train_labels), yticklabels=np.unique(train_labels))
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""**CNN + BiLSTM**

*CNN + BiLSTM on Word2Vec*
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from gensim.models import Word2Vec
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt  # Import for plotting
import seaborn as sns           # Import for heatmap visualization

# Load Word2Vec model
word2vec_model_path = '/content/drive/MyDrive/ED/Datasets/word2vec_bengali.model'
word2vec_model = Word2Vec.load(word2vec_model_path)
embedding_matrix = word2vec_model.wv.vectors
vocab_size, embedding_dim = embedding_matrix.shape

# Model parameters
max_length = 100
filter_size = 3
num_units_cnn = 64
num_units_lstm = 32
batch_size = 16
learning_rate = 0.001
epochs = 30

# Load datasets
train_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Prepare text data
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(train_df['Tokenized_Text'])
def get_sequences(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')
    return padded
train_sequences = get_sequences(train_df['Tokenized_Text'])
test_sequences = get_sequences(test_df['Tokenized_Text'])
val_sequences = get_sequences(val_df['Tokenized_Text'])

# Labels
train_labels = train_df['Label'].values
test_labels = test_df['Label'].values
val_labels = val_df['Label'].values

# Define CNN + BiLSTM model
model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Conv1D(num_units_cnn, filter_size, activation='relu'),
    MaxPooling1D(pool_size=2),
    Bidirectional(LSTM(num_units_lstm)),
    Dense(len(np.unique(train_labels)), activation='softmax')
])

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
history = model.fit(train_sequences, train_labels, epochs=epochs, validation_data=(val_sequences, val_labels), batch_size=batch_size)

# Evaluate the model on test data
predictions = model.predict(test_sequences)
predicted_labels = np.argmax(predictions, axis=1)

# Metrics
accuracy = accuracy_score(test_labels, predicted_labels)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(test_labels, predicted_labels))

# Calculate the confusion matrix
conf_mat = confusion_matrix(test_labels, predicted_labels)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Reds', xticklabels=np.unique(train_labels), yticklabels=np.unique(train_labels))
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""*CNN + BiLSTM on FastText*"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from gensim.models import FastText
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt  # Import for plotting
import seaborn as sns           # Import for heatmap visualization

# Load FastText model and its vectors
fasttext_model_path = '/content/drive/MyDrive/ED/Datasets/fasttext_bengali.model'
fasttext_model = FastText.load(fasttext_model_path)
embedding_matrix = np.load('/content/drive/MyDrive/ED/Datasets/fasttext_bengali.model.wv.vectors_ngrams.npy')
vocab_size, embedding_dim = embedding_matrix.shape

# Model parameters
max_length = 100
filter_size = 3
num_units_cnn = 64
num_units_lstm = 32
batch_size = 16
learning_rate = 0.001
epochs = 30

# Load datasets
train_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/train_dataset.xlsx')
test_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/test_dataset.xlsx')
val_df = pd.read_excel('/content/drive/MyDrive/ED/Datasets/val_dataset.xlsx')

# Prepare text data
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(train_df['Tokenized_Text'])

def get_sequences(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')
    return padded

train_sequences = get_sequences(train_df['Tokenized_Text'])
test_sequences = get_sequences(test_df['Tokenized_Text'])
val_sequences = get_sequences(val_df['Tokenized_Text'])

# Labels
train_labels = train_df['Label'].values
test_labels = test_df['Label'].values
val_labels = val_df['Label'].values

# Define CNN + BiLSTM model
model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Conv1D(num_units_cnn, filter_size, activation='relu'),
    MaxPooling1D(pool_size=2),
    Bidirectional(LSTM(num_units_lstm)),
    Dense(len(np.unique(train_labels)), activation='softmax')
])

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
history = model.fit(train_sequences, train_labels, epochs=epochs, validation_data=(val_sequences, val_labels), batch_size=batch_size)

# Evaluate the model on test data
predictions = model.predict(test_sequences)
predicted_labels = np.argmax(predictions, axis=1)

# Metrics
accuracy = accuracy_score(test_labels, predicted_labels)
print(f"\nAccuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(test_labels, predicted_labels))

# Calculate the confusion matrix
conf_mat = confusion_matrix(test_labels, predicted_labels)

# Create a heatmap from the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Greens', xticklabels=np.unique(train_labels), yticklabels=np.unique(train_labels))
plt.title('Confusion Matrix Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()





